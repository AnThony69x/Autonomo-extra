
# ğŸ¤– MÃ³dulo de Agente AutÃ³nomo con Flujos - AnÃ¡lisis de Texto con Gemini AI

Este mÃ³dulo es una expansiÃ³n del sistema `ExposIA` desarrollada con NestJS. Implementa un agente inteligente autÃ³nomo que orquesta un flujo automÃ¡tico de anÃ¡lisis de texto, utilizando **Gemini (Google Generative AI)**, y persiste los resultados en un endpoint externo.

---

## ğŸš€ Funcionalidad

Este mÃ³dulo ejecuta un flujo en 3 pasos automÃ¡ticos:

1. **ObtenciÃ³n de texto** desde un endpoint externo (mock server).
2. **AnÃ¡lisis del texto** usando un modelo LLM (Gemini API).
3. **Persistencia de resultados** en un segundo endpoint vÃ­a POST.

---

## ğŸ§¾ JustificaciÃ³n del Uso de Mock Server como TecnologÃ­a de Flujo

Para cumplir con el requerimiento de utilizar una **tecnologÃ­a externa de flujo**, se empleÃ³ **Postman Mock Server**. Esta herramienta permite simular de forma realista el comportamiento de endpoints externos de otros microservicios del grupo, sin depender de que estÃ©n en ejecuciÃ³n.

Con esto se logra:

- Orquestar un flujo **totalmente automatizado y externo** al mÃ³dulo.
- **Separar responsabilidades** entre microservicios de forma limpia.
- Simular condiciones reales de producciÃ³n sin necesidad de dependencias activas.
- Cumplir con el requerimiento de consumir **otro microservicio del grupo**, usando URLs pÃºblicas.

---

## ğŸ”§ TecnologÃ­as Utilizadas

- ğŸ§  Google Generative AI (Gemini API)
- ğŸŒ NestJS (TypeScript)
- ğŸ§ª Postman (Mock Server para endpoints simulados)
- âš™ï¸ Axios / HttpService para integraciÃ³n HTTP
- ğŸ“¦ dotenv / ConfigService para variables de entorno

---

## ğŸ§± Estructura del Proyecto

```
src/
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ conversacion-ia.controller.ts
â”œâ”€â”€ services/
â”‚   â””â”€â”€ conversacion-ia.service.ts
â”œâ”€â”€ dtos/
â”‚   â””â”€â”€ consulta.dto.ts
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ conversacion-ia.module.ts
```

---

## ğŸ“Œ Endpoints

### 1. `POST /v1/start-analysis-agent`
Activa el agente autÃ³nomo. El flujo realiza:

- GET a `/consulta-texto` (mock server)
- AnÃ¡lisis del contenido con Gemini
- POST a `/guardar-respuesta` (mock server)

**Respuesta esperada:**
```json
{
  "fuente_texto": "https://mockserver.com/consulta-texto",
  "pregunta_enviada_a_ia": "Texto recibido...",
  "respuesta_ia": "Respuesta generada por Gemini...",
  "estado": "ok"
}
```

---

### 2. `POST /v1/consultar-prueba-ia`
Permite probar directamente el anÃ¡lisis de texto enviando un cuerpo manual:

**Body:**
```json
{
  "texto": "Texto a evaluar por la IA"
}
```

**Respuesta:**
```json
{
  "pregunta": "Texto a evaluar por la IA",
  "respuesta_ia": "EvaluaciÃ³n detallada de Gemini",
  "estado": "ok"
}
```

---

## âš™ï¸ Requisitos de Entorno (`.env`)

```env
GEMINI_API_KEY=tu_api_key_de_google
GEMINI_MODEL=models/gemini-pro
PROMPT_REVISION_BASE=EvalÃºa el siguiente texto motivacional segÃºn criterios acadÃ©micos y da sugerencias de mejora.
```

---

## ğŸ“¦ InstalaciÃ³n y EjecuciÃ³n

1. Instalar dependencias:
```bash
npm install
```

2. Configurar `.env` con tu API Key de Google AI

3. Ejecutar el servidor:
```bash
npm run start:dev
```

---

## ğŸ§ª SimulaciÃ³n de Microservicios con Mock Server (Postman)

- **GET** `https://<mock>.mock.pstmn.io/consulta-texto`  
Devuelve un texto motivacional.

- **POST** `https://<mock>.mock.pstmn.io/guardar-respuesta`  
Recibe los datos generados por el LLM.

---

## ğŸ“Š Ejemplo del Flujo del Agente

```mermaid
graph TD
    A[POST /start-analysis-agent] --> B[GET /consulta-texto (Mock)]
    B --> C[Procesamiento Gemini API]
    C --> D[POST /guardar-respuesta (Mock)]
    D --> E[Respuesta Final al Cliente]
```

